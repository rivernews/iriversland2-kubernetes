# based on example: https://github.com/confluentinc/kafka-connect-elasticsearch/blob/master/config/quickstart-elasticsearch.properties

# all properties: https://docs.confluent.io/current/connect/kafka-connect-elasticsearch/configuration_options.html#connector
connection.url=http://elasticsearch-master.kube-system.svc.cluster.local:9200
connector.class=io.confluent.connect.elasticsearch.ElasticsearchSinkConnector
tasks.max=1

 # TODO: we can decide this
name=elasticsearch-sink
# topics=test-elasticsearch-sink # TODO: topic names are based on tables, so will have multiple names. is there a `*` to match all? --> seems like SMT should handle this
# TODO: type under which the events will be registered in Elasticsearch. can we decide this?
key.ignore=false
behavior.on.null.values=delete

#### examples for hint ####

# debezium blog: https://debezium.io/blog/2018/01/17/streaming-to-elasticsearch/

# consume every topics from postgres source connector
# to check all current topics stored in kafka, run:
# $KAFKA_HOME/bin/kafka-topics.sh --list --bootstrap-server kafka-stack-release-0.kafka-stack-release-headless.kube-system.svc.cluster.local:9092

# Elasticsearch mapping name. Gets created automatically if doesn't exist
# type.name=kafka-connect
type.name=api_company

# Which topic to stream data from into Elasticsearch
topics.regex=^appl_tracky__postgres\.public\.api_company$

# SMT (Single message transformation)
# transforms=unwrap,key
# transforms.unwrap.type=io.debezium.transforms.UnwrapFromEnvelope
transforms=unwrap,key
transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState
transforms.unwrap.drop.tombstones=false
transforms.unwrap.delete.handling.mode=rewrite
# transforms.unwrap.add.source.fields=table,lsn
transforms.key.type=org.apache.kafka.connect.transforms.ExtractField$Key
transforms.key.field=id

# Q: but how do we include all tables in database?

# debezium postgres connector topic/table naming convention:
# https://debezium.io/documentation/reference/0.10/connectors/postgresql.html#topic-names
#
# format:
# <serverName == logical name of the connector == database.server.name>.<schema default is `public`>.<table name>

# Solution 1 `RegexRouter`: https://debezium.io/blog/2017/09/25/streaming-to-another-database/#topic_naming
# the post is a bit old but does illustrate the solution using RegexRouter
# another example using RegexRouter, by confluent doc: https://docs.confluent.io/current/connect/transforms/regexrouter.html#regexrouter
# a github issue example using RegexRouter & topic to filter out and do topic/index replacement: https://github.com/confluentinc/kafka-connect-elasticsearch/issues/298#issuecomment-553344534

# Solution 2 `kcql`: https://stackoverflow.com/questions/48942326/kafka-connect-multiple-topics-in-sink-connector-properties
# but we need to figure out how to use regex with kcql